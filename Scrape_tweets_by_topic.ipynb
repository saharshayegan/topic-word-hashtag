{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scrape tweets by topic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B9PmSdBcxGGM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef1c7cf5-7c71-46f8-95a6-678d0828d6cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [76.0 kB]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [872 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:14 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [738 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:17 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [771 kB]\n",
            "Hit:19 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,517 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,823 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,954 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,463 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,242 kB]\n",
            "Get:25 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [934 kB]\n",
            "Get:26 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Fetched 14.7 MB in 8s (1,942 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.8-minimal libpython3.8-stdlib python3.8-minimal\n",
            "Suggested packages:\n",
            "  python3.8-venv binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.8-minimal libpython3.8-stdlib python3.8 python3.8-minimal\n",
            "0 upgraded, 4 newly installed, 0 to remove and 64 not upgraded.\n",
            "Need to get 4,676 kB of archives.\n",
            "After this operation, 18.5 MB of additional disk space will be used.\n",
            "Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 libpython3.8-minimal amd64 3.8.12-1+bionic3 [762 kB]\n",
            "Get:2 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 python3.8-minimal amd64 3.8.12-1+bionic3 [1,825 kB]\n",
            "Get:3 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 libpython3.8-stdlib amd64 3.8.12-1+bionic3 [1,656 kB]\n",
            "Get:4 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 python3.8 amd64 3.8.12-1+bionic3 [433 kB]\n",
            "Fetched 4,676 kB in 8s (586 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython3.8-minimal:amd64.\n",
            "(Reading database ... 155229 files and directories currently installed.)\n",
            "Preparing to unpack .../libpython3.8-minimal_3.8.12-1+bionic3_amd64.deb ...\n",
            "Unpacking libpython3.8-minimal:amd64 (3.8.12-1+bionic3) ...\n",
            "Selecting previously unselected package python3.8-minimal.\n",
            "Preparing to unpack .../python3.8-minimal_3.8.12-1+bionic3_amd64.deb ...\n",
            "Unpacking python3.8-minimal (3.8.12-1+bionic3) ...\n",
            "Selecting previously unselected package libpython3.8-stdlib:amd64.\n",
            "Preparing to unpack .../libpython3.8-stdlib_3.8.12-1+bionic3_amd64.deb ...\n",
            "Unpacking libpython3.8-stdlib:amd64 (3.8.12-1+bionic3) ...\n",
            "Selecting previously unselected package python3.8.\n",
            "Preparing to unpack .../python3.8_3.8.12-1+bionic3_amd64.deb ...\n",
            "Unpacking python3.8 (3.8.12-1+bionic3) ...\n",
            "Setting up libpython3.8-minimal:amd64 (3.8.12-1+bionic3) ...\n",
            "Setting up python3.8-minimal (3.8.12-1+bionic3) ...\n",
            "Setting up libpython3.8-stdlib:amd64 (3.8.12-1+bionic3) ...\n",
            "Setting up python3.8 (3.8.12-1+bionic3) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "update-alternatives: using /usr/bin/python3.8 to provide /usr/bin/python3 (python3) in auto mode\n",
            "Python 3.8.12\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2108k  100 2108k    0     0  12.1M      0 --:--:-- --:--:-- --:--:-- 12.1M\n",
            "Collecting pip\n",
            "  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
            "     |████████████████████████████████| 1.7 MB 4.4 MB/s            \n",
            "\u001b[?25hCollecting setuptools\n",
            "  Downloading setuptools-60.5.0-py3-none-any.whl (958 kB)\n",
            "     |████████████████████████████████| 958 kB 67.9 MB/s            \n",
            "\u001b[?25hCollecting wheel\n",
            "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: wheel, setuptools, pip\n",
            "Successfully installed pip-21.3.1 setuptools-60.5.0 wheel-0.37.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n",
            "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to /tmp/pip-req-build-rdjh_vat\n",
            "  Running command git clone --filter=blob:none -q https://github.com/JustAnotherArchivist/snscrape.git /tmp/pip-req-build-rdjh_vat\n",
            "  Resolved https://github.com/JustAnotherArchivist/snscrape.git to commit 7f8867825330ee9368ff3284d02033e3e58ab90b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting requests[socks]\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "     |████████████████████████████████| 63 kB 1.1 MB/s             \n",
            "\u001b[?25hCollecting lxml\n",
            "  Downloading lxml-4.7.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.9 MB)\n",
            "     |████████████████████████████████| 6.9 MB 8.1 MB/s            \n",
            "\u001b[?25hCollecting beautifulsoup4\n",
            "  Downloading beautifulsoup4-4.10.0-py3-none-any.whl (97 kB)\n",
            "     |████████████████████████████████| 97 kB 5.3 MB/s             \n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.4.2-py3-none-any.whl (9.9 kB)\n",
            "Collecting pytz\n",
            "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
            "     |████████████████████████████████| 503 kB 72.5 MB/s            \n",
            "\u001b[?25hCollecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.3.1-py3-none-any.whl (37 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
            "     |████████████████████████████████| 61 kB 6.9 MB/s             \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "     |████████████████████████████████| 138 kB 66.6 MB/s            \n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
            "     |████████████████████████████████| 149 kB 64.5 MB/s            \n",
            "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
            "  Downloading charset_normalizer-2.0.10-py3-none-any.whl (39 kB)\n",
            "Collecting PySocks!=1.5.7,>=1.5.6\n",
            "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: snscrape\n",
            "  Building wheel for snscrape (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for snscrape: filename=snscrape-0.4.3.20220107.dev8+g7f88678-py3-none-any.whl size=59385 sha256=cd468446f99e2e4f68abeed505f3877f682a84e4fc1a3bea0f4f75207691ca76\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j4xw872d/wheels/92/42/87/33fa9b18f7a75d02643a9ca3743339aec9be28c6796267c7d8\n",
            "Successfully built snscrape\n",
            "Installing collected packages: urllib3, idna, charset-normalizer, certifi, soupsieve, requests, PySocks, pytz, lxml, filelock, beautifulsoup4, snscrape\n",
            "Successfully installed PySocks-1.7.1 beautifulsoup4-4.10.0 certifi-2021.10.8 charset-normalizer-2.0.10 filelock-3.4.2 idna-3.3 lxml-4.7.1 pytz-2021.3 requests-2.27.1 snscrape-0.4.3.20220107.dev8+g7f88678 soupsieve-2.3.1 urllib3-1.26.8\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting pandas\n",
            "  Downloading pandas-1.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "     |████████████████████████████████| 11.7 MB 4.3 MB/s            \n",
            "\u001b[?25hCollecting numpy>=1.18.5\n",
            "  Downloading numpy-1.22.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "     |████████████████████████████████| 16.8 MB 57.4 MB/s            \n",
            "\u001b[?25hCollecting python-dateutil>=2.8.1\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "     |████████████████████████████████| 247 kB 72.6 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2021.3)\n",
            "Collecting six>=1.5\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: six, python-dateutil, numpy, pandas\n",
            "Successfully installed numpy-1.22.1 pandas-1.4.0 python-dateutil-2.8.2 six-1.16.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datetime\n",
            "  Downloading DateTime-4.3-py2.py3-none-any.whl (60 kB)\n",
            "     |████████████████████████████████| 60 kB 2.5 MB/s             \n",
            "\u001b[?25hCollecting zope.interface\n",
            "  Downloading zope.interface-5.4.0-cp38-cp38-manylinux2010_x86_64.whl (259 kB)\n",
            "     |████████████████████████████████| 259 kB 8.5 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from datetime) (2021.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from zope.interface->datetime) (60.5.0)\n",
            "Installing collected packages: zope.interface, datetime\n",
            "Successfully installed datetime-4.3 zope.interface-5.4.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (2021.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting tweepy\n",
            "  Downloading tweepy-4.5.0-py2.py3-none-any.whl (66 kB)\n",
            "     |████████████████████████████████| 66 kB 2.1 MB/s             \n",
            "\u001b[?25hCollecting requests-oauthlib<2,>=1.0.0\n",
            "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.8/dist-packages (from tweepy) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.27.0->tweepy) (2.0.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.27.0->tweepy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.27.0->tweepy) (1.26.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
            "     |████████████████████████████████| 146 kB 9.0 MB/s            \n",
            "\u001b[?25hInstalling collected packages: oauthlib, requests-oauthlib, tweepy\n",
            "Successfully installed oauthlib-3.1.1 requests-oauthlib-1.3.0 tweepy-4.5.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "     |████████████████████████████████| 316 kB 4.2 MB/s            \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "     |████████████████████████████████| 1.4 MB 28.6 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "     |████████████████████████████████| 233 kB 53.6 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from nltk==3.3->hazm) (1.16.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394488 sha256=f8d6297753f32e18472798d388d5ff10763e7ca6ed2cf4c8777043c4c6125dfb\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/1d/3a/0a8c14c30132b4f9ffd796efbb6746f15b3d6bcfc1055a9346\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp38-cp38-linux_x86_64.whl size=154027 sha256=a0ac21d26c7919e41f949b054c8ce139d4f68b2c2483a75375ed7468c91fad2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/d8/9f/59fd78b2b7d1e9ffcb68fb6de80c2e7c20b804c8cbc4d8fc23\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update \n",
        "!sudo apt-get install python3.8 \n",
        "\n",
        "#change alternatives\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 2\n",
        "\n",
        "#check python version\n",
        "!python --version\n",
        "\n",
        "! curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py \n",
        "! python3 get-pip.py --force-reinstall \n",
        "\n",
        "! pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
        "\n",
        "! pip install pandas \n",
        "! pip install datetime\n",
        "! pip install pytz\n",
        "! pip install tweepy\n",
        "! pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prepared_code = \"\"\"\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "from random import random\n",
        "from datetime import date\n",
        "from multiprocessing.dummy import Pool\n",
        "import time\n",
        "\n",
        "# https://github.com/JustAnotherArchivist/snscrape/blob/master/snscrape/modules/twitter.py\n",
        "\n",
        "class Twitter_scraper:\n",
        "  \n",
        "  def __init__(self,\n",
        "               max_results: int,\n",
        "               all_words = [],        # Example: ['what’s', 'happening'] · contains both “what’s” and “happening”\n",
        "               exact_pharase=[],      # Example: ['happy hour'] · contains the exact phrase “happy hour”\n",
        "               any_words = [],        # Example: ['cats', 'dogs'] · contains either “cats” or “dogs” (or both)\n",
        "               none_words = [],       # Example: ['cats', 'dogs'] · does not contain “cats” and does not contain “dogs”\n",
        "               hashtags = [],         # Example: ['#ThrowbackThursday'] or ['ThrowbackThursday'] · contains the hashtag #ThrowbackThursday\n",
        "               mentioned_users = [],  # Example: ['@SFBART', '@Caltrain'] or ['SFBART', 'Caltrain'] · mentions @SFBART or mentions @Caltrain\n",
        "               from_users = [],       # Example: ['@Twitter'] or ['Twitter'] · sent from @Twitter\n",
        "               to_users = [],         # Example: ['@Twitter'] or ['Twitter'] · sent in reply to @Twitter\n",
        "               with_links = True,\n",
        "               with_replies = True,\n",
        "               **kwargs):\n",
        "    \n",
        "    self.number_of_user = 0\n",
        "    self.max_results = max_results\n",
        "    self.all_words = Twitter_scraper.all_of_these_words(all_words)\n",
        "    self.exact_pharase = f'\\\"{exact_pharase}\\\"' if exact_pharase else ''\n",
        "    self.any_words = Twitter_scraper.any_of_these_words(any_words)\n",
        "    self.none_words = Twitter_scraper.none_of_these_words(none_words)\n",
        "    self.these_hashtags = Twitter_scraper.any_of_these_hashtags(hashtags)\n",
        "    self.mentioned_users = Twitter_scraper.mentioning_these_users(mentioned_users)\n",
        "    self.with_links = f'-filter:links' if not with_links else ''\n",
        "    self.with_replies = f'-filter:replies' if not with_replies else ''\n",
        "\n",
        "    self.query_dict = {'all_words':self.all_words, 'exact_pharase':self.exact_pharase,\n",
        "                  'any_words':self.any_words, 'none_words':self.none_words, 'these_hashtags':self.these_hashtags,\n",
        "                  'mentioned_users':self.mentioned_users, 'with_links': self.with_links, 'with_replies':self.with_replies}\n",
        "\n",
        "    self.query_dict['from'] = Twitter_scraper.f_or_t_users(from_users, 'from')\n",
        "    self.query_dict['to'] = Twitter_scraper.f_or_t_users(to_users, 'to')\n",
        "\n",
        "    for key, value in kwargs.items():\n",
        "        self.query_dict[key] = (f'({key}:{value})')\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def f_or_t_users(users, key):\n",
        "    if not users:\n",
        "      return ''\n",
        "    tmp_list = [f'{key}:{user}' for user in users]\n",
        "    return('(' + ' OR '.join(tmp_list) + ')')\n",
        "\n",
        "  @staticmethod\n",
        "  def all_of_these_words(all_words):\n",
        "    if not all_words:\n",
        "      return ''\n",
        "    return ' '.join(all_words)\n",
        "\n",
        "  @staticmethod\n",
        "  def any_of_these_words(any_words):\n",
        "    if not any_words:\n",
        "      return ''\n",
        "    return ('(' + ' OR '.join(any_words) + ')')\n",
        "\n",
        "  @staticmethod\n",
        "  def none_of_these_words(none_words):\n",
        "    if not none_words:\n",
        "      return ''    \n",
        "    return ('-' + ' -'.join(none_words))\n",
        "\n",
        "  @staticmethod\n",
        "  def any_of_these_hashtags(hashtags):\n",
        "    if not hashtags:\n",
        "      return ''    \n",
        "    tmp_list = ['#'+ h.replace('#','') for h in hashtags]\n",
        "    return ('(' + ' OR '.join(tmp_list) + ')')\n",
        "\n",
        "  @staticmethod\n",
        "  def mentioning_these_users(users):\n",
        "    if not users:\n",
        "      return ''    \n",
        "    tmp_list = ['@'+ h.replace('@','') for h in users]\n",
        "    return ('(' + ' OR '.join(tmp_list) + ')')\n",
        "\n",
        "  def create_query(self, query_dict):\n",
        "    tmp_string = ''\n",
        "    res = dict([(key, val) for key, val in \n",
        "           query_dict.items() if val])\n",
        "    del query_dict\n",
        "    query = ' '.join(res.values())\n",
        "    del res\n",
        "\n",
        "    return query\n",
        "\n",
        "\n",
        "  def crawler(self, query, error_counter=0):\n",
        "    # Creating list to append tweet data\n",
        "    tweets_list = []\n",
        "    try:\n",
        "      # Using TwitterSearchScraper to scrape data and append tweets to list\n",
        "      scraper = sntwitter.TwitterSearchScraper(query)\n",
        "      i = 0\n",
        "      for tweet in scraper.get_items(): #declare a username\n",
        "          if i >= self.max_results: #check number and date\n",
        "            break\n",
        "\n",
        "          tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.replyCount, tweet.retweetCount,\n",
        "                tweet.likeCount, tweet.user.username, tweet.lang, tweet.media, tweet.hashtags]) #declare the attributes to be returned\n",
        "          i += 1\n",
        "      \n",
        "    except Exception as e:\n",
        "      if 'Unable to find guest token' in str(e):\n",
        "          error_counter += 1\n",
        "          if error_counter > 3:\n",
        "            error_counter = 0\n",
        "            print(\"Sleep Time!\")\n",
        "            time.sleep(30.3 *60)\n",
        "            print(\"Morning!\")\n",
        "\n",
        "          return self.crawler(query, error_counter)\n",
        "\n",
        "      print(f\"query: {query} , {e}\")\n",
        "\n",
        "    # Creating a dataframe from the tweets list above \n",
        "    tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Reply Count',\n",
        "                                                    'Retweet Count', 'Like Count', 'Username', 'Lang', 'Media', 'Hashtags'])\n",
        "    return tweets_df\n",
        "\n",
        "\n",
        "  def basic_mode(self):\n",
        "    query = self.create_query(self.query_dict)\n",
        "    return self.crawler(query)\n",
        "\n",
        "  def user_crawler(self, user):\n",
        "    tmp_dict = self.query_dict.copy()\n",
        "    tmp_dict['from'] = (f'(from:{user})')\n",
        "    query = self.create_query(tmp_dict)\n",
        "    del tmp_dict\n",
        "    return self.crawler(query)\n",
        "\n",
        "\n",
        "  def user_mode(self, user_list):   \n",
        "    user_crawler = self.user_crawler \n",
        "    pool = Pool(22)\n",
        "    df_list = pool.map(user_crawler, user_list)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    result_df = pd.concat(df_list, ignore_index=True)\n",
        "    return result_df\n",
        "\n",
        "\n",
        "\n",
        "topic = 'sports'\n",
        "none_words=[] #['zodiac','bubble']\n",
        "from_users = ['khabaronlinee', 'ilnanews', 'EtemadOnline', 'SharghDaily',\n",
        "'hamshahrinews','PadDolat', 'IranNewspaper', 'isna_farsi', 'FarsNews_Agency',\n",
        "'yjc___agency', 'Entekhab_News', 'IRNA_1313', 'Tasnimnews_Fa']\n",
        " \n",
        "all_words=[]\n",
        "any_words=['ورزش','فوتبال','لیگ','المپیک','والیبال','بسکتبال','مسابقات','تیم','فدراسیون','جام','مربی','بازیکنان','ورزشکاران']\n",
        "until = '2022-01-27'\n",
        "since = '2020-01-27'\n",
        "scraper = Twitter_scraper(max_results=10**4, all_words=all_words,any_words=any_words,none_words=none_words,until=until, since=since, lang=\"fa\", from_users=from_users, with_replies=False)\n",
        "result = scraper.basic_mode()\n",
        "\n",
        "result.to_csv('./drive/MyDrive/Colab Notebooks/topic_words/'+topic+'_tweets.csv', index=False)\n",
        "print(result.shape)\n",
        "\n",
        "# Example:\n",
        "#all_words = ['ایران', 'است'] # contains both “ایران” and “است”\n",
        "#lang=\"fa\"\n",
        "#scraper = Twitter_scraper(max_results=10**11, until=\"2020-01-01\", since=\"2019-01-01\", lang=lang, with_replies=False)\n",
        "#result = scraper.basic_mode()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(\"python.py\", \"w\") as f:\n",
        "  f.write(prepared_code)\n",
        "\n"
      ],
      "metadata": {
        "id": "2LDAEJjaxNqN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python python.py"
      ],
      "metadata": {
        "id": "glB1gSrrxNcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a408f1e5-52da-4e13-ff3c-229d308a3a23"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79uzAFWN4nda",
        "outputId": "89d6b815-6fbb-4577-8e14-420409501982"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "economics\n",
        "\n",
        "any_words=['تجارت','مالی','اقتصادی','بورس','سکه','ارز','مالیات','قیمت','وام','اقتصاد']\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "health\n",
        "\n",
        "any_words=['دارو','پاندمی','واگیر','مریضی','پزشکی','پزشکان','پزشک','بهداشت','درمان','بیماری','ویروس','واکسن','همه‌گیری','کرونا', 'بیمارستان', 'درمانگاه', 'بهداشتی', 'درمانی', 'تندرستی', 'دندانپزشک', 'فایزر','سینوفارم','آسترازنکا','برکت']\n",
        "\n",
        "any_words=['دارو','پاندمی','واگیر','مریضی','پزشکی','پزشک','بهداشت','درمان','بیماری','واکسن','کرونا', 'بیمارستان', 'درمانگاه', 'بهداشتی', 'تندرستی', 'فایزر','سینوفارم']\n",
        "\n",
        "any_words=['دارو','پاندمی','پزشکی','پزشک','بهداشت','درمان','بیماری','واکسن','کرونا', 'بهداشتی', 'تندرستی', 'فایزر','سینوفارم']\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "sports\n",
        "\n",
        "any_words=['ورزش','فوتبال','لیگ','المپیک','والیبال','بسکتبال','مسابقات','تیم','فدراسیون','جام','مربی','بازیکنان','ورزشکاران']\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "art\n",
        "\n",
        "any_words=['سینما','ادبیات','موسیقی','نقاشی','شعر','فیلم','بازیگر','هنرمند','هنر','هنرمندان']\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "tech\n",
        "\n",
        "any_words=['تکنولوژی','فناوری','بیت کوین','بیت‌کوین','رمزارز','دیجیتال','اینترنت','شبکه','هک','رایانه','رایانه‌ای','بازی','موبایل']\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "transport\n",
        "\n",
        "any_words=['جاده','اتوبان','بنزین','تاکسی','اتوبوس','مترو','هواپیما','هواپیمایی','ناوگان','قطار','پرواز','خودرو','ترافیک']\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "education\n",
        "\n",
        "any_words=['مدرسه','معلم','معلمان','دبیر','دبیران','مدارس','دانشگاه','دانشگاه‌ها','دانش‌آموزان','کنکور','دانشجویان','دانشجو','اساتید']\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "religion\n",
        "\n",
        "any_words=['اسلام','قرآن','نماز','مسلمان','مسلمانان','شیعه','سنی','شیعیان','مسیحیت','یهودیت','ائمه','پیامبر']\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "geo\n",
        "\n",
        "any_words=['خشکسالی','آلودگی','سیل','زلزله','برف','باران','بارش','بارنگی','بی‌آبی','هواشناسی','رانش','آفتابی','گرما']\n",
        "\n",
        "---\n",
        "\n",
        "social\n",
        "\n",
        "any_words=['','','','','','','','','','','','','']\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "life_style\n",
        "\n",
        "any_words=['','','','','','','','','','','','','']\n"
      ],
      "metadata": {
        "id": "pHaZeqQAvTMZ"
      }
    }
  ]
}